[["linear-regression.html", "Chapter 2 Linear Regression 2.1 Select Alpha Level 2.2 Create Linear Model of All Variables 2.3 Global F-Test 2.4 Assumptions 2.5 Influential Observations / Outliers 2.6 Variable Selection 2.7 Model Evaluation 2.8 Summarize Findings", " Chapter 2 Linear Regression 2.1 Select Alpha Level snippet fsr_alpha There are three ways to select an alpha level prior to modeling. You can stick with the standard 0.05 cutoff, use the below sample size table (from a paper by Raftery), or use the below calculation based on sample size. 2.2 Create Linear Model of All Variables Assuming you have a reasonable number of variables, start by putting them all into a model to predict the target. Variable selection/ reduction will occur later in the process. If you have a large number of variables, you can consider variable reduction or a different analytical technique. 2.3 Global F-Test snippet fsr_globalF Once the linear model is created, a Global F test will discern if any of the variables are significant in predicting y. If no variables are significant, you should re-evaluate the variables or the model selected for this business problem. Assuming at least one variable is significant, continue with the modeling process. 2.4 Assumptions Next, you need to assess a number of assumptions that come with linear regression modeling. First evaluate if the predictor variables have a linear relationship with the response by evaluating their shape when plotted. If the variable looks like a quadratic (curve), polynomial, or not straight consider going back and creating higher order terms. 2.4.1 Linearity snippet fsr_assumptions_independence Check if they are linear (yes, keep moving) (No) If they look polynomial/ quadratic/ etc. reconsider higher order terms 2.4.2 Independent Random Errors snippet fsr_assumptions_independence Next check is the errors are independent by inspecting if the residuals show a pattern. This can also be determined statistically with the Durbin-Watson test. 2.4.3 Random Error Constant Variance snippet fsr_assumptions_constvar Then you need to further inspect the residuals by looking for patterns. If the residuals visually have a constant variance, then the random error assumption is met. This can also be evaluated statistically with the Spearman Rank Correlation. Values close to 0 mean the variance is constant. If it’s not constant (visually or a spearman rank correlation close to 1 or -1) consider alternative options such as weighted least squares, transforming the data, or a different distribution (ex: Poisson) 2.4.4 Random Errors Normally Distributed snippet fsr_assumptions_normdist Next, there are three ways to check that the random errors are normally distribution. You can evaluate a histogram of the residuals, create a qq plot and look for a straight line, or do a formal test. The formal test is either Anderson-Darling or Shapiro-Wilk. Shapiro-Wilk is commonly used for smaller data sets. If the errors are not normal, consider a transformation such as box-cox or a log of the x’s. 2.4.5 Single Predictor? The final assumption to check is multicollinearity. If you only have 1 predictor, you can skip this step. 2.4.6 Multiple Predictors 2.4.6.1 Check for Multicollinearity snippet fsr_assumptions_multicol If you have multiple predictors check multicollinearity with VIF. If VIF is greater than 10, there is multicollinearity and you should remove the related variable or transform the variable. If the VIF is less than 10, continue on to influential observations/ outliers. 2.5 Influential Observations / Outliers Once all of the assumptions are met, consider handling influential observations and outliers. The strategies below to help identify influential observations, and 5 strategies to handle them once detected. 2.5.1 Identify influential observations snippet fsr_influential Internally Studentized residuals (good for detecting outliers) Externally Studentized residuals (good for detecting outliers) Cook’s D (good for detecting influential observations) DFFITS (good for detecting influential observations) DFBETAS (good for detecting influential observations) Hat values (good for detecting influential observations) 2.5.2 Handle Influential Observations snippet fsr_outliers Recheck the data to ensure that no transcription or data entry errors occurred. If the data is valid, one possible explanation is that the model is not adequate. A model with higher-order terms, such as polynomials and interactions between the variables, might be necessary to fit the data well. Nonlinear model Determine the robustness of the inference bv running the analysis both with and without the influential observations. Robust Regression (Covered Later in Program) Weighted Least Squares (WCS) 2.6 Variable Selection snippet fsr_varselect_linreg When the data is postured and ready for analysis, start with variable selection. At this point you can choose between many selection techniques such as forward, backward, stepwise, and regularization (L1, L2, or elastic net). If you have a lot of variable you’re looking to cut back on, consider backward selection or variable clustering techniques. You also need to choose a model evaluation statistic of AIC, BIC, or p-value. 2.7 Model Evaluation snippet fsr_modeval After the variables are selected, it’s time to run the linear regression. Evaluation metrics such as MAE, MAPE, RMSE, and MSE can be computed with the model output. Additionally, the chosen evaluation statistic (AIC, BIC, or adjusted r^2) and be computed on the validation set for comparison and on the test set (or the test rolled into the rest of the data) to report the final metrics. 2.8 Summarize Findings With these final metrics, you can summarize any business insights, findings, and takeaways based on the influential variables, model metrics, and business context. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
