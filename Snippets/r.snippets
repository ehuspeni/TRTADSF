snippet lib
	library(${1:package})

snippet req
	require(${1:package})

snippet src
	source("${1:file.R}")

snippet ret
	return(${1:code})

snippet mat
	matrix(${1:data}, nrow = ${2:rows}, ncol = ${3:cols})

snippet sg
	setGeneric("${1:generic}", function(${2:x, ...}) {
		standardGeneric("${1:generic}")
	})

snippet sm
	setMethod("${1:generic}", ${2:class}, function(${2:x, ...}) {
		${0}
	})

snippet sc
	setClass("${1:Class}", slots = c(${2:name = "type"}))

snippet if
	if (${1:condition}) {
		${0}
	}

snippet el
	else {
		${0}
	}

snippet ei
	else if (${1:condition}) {
		${0}
	}

snippet fun
	${1:name} <- function(${2:variables}) {
		${0}
	}

snippet for
	for (${1:variable} in ${2:vector}) {
		${0}
	}

snippet while
	while (${1:condition}) {
		${0}
	}

snippet switch
	switch (${1:object},
		${2:case} = ${3:action}
	)

snippet apply
	apply(${1:array}, ${2:margin}, ${3:...})

snippet lapply
	lapply(${1:list}, ${2:function})

snippet sapply
	sapply(${1:list}, ${2:function})

snippet mapply
	mapply(${1:function}, ${2:...})

snippet tapply
	tapply(${1:vector}, ${2:index}, ${3:function})

snippet vapply
	vapply(${1:list}, ${2:function}, FUN.VALUE = ${3:type}, ${4:...})

snippet rapply
	rapply(${1:list}, ${2:function})

snippet ts
	`r paste("#", date(), "------------------------------\n")`

snippet shinyapp
	library(shiny)
	
	ui <- fluidPage(
	  ${0}
	)
	
	server <- function(input, output, session) {
	  
	}
	
	shinyApp(ui, server)

snippet shinymod
	${1:name}_UI <- function(id) {
	  ns <- NS(id)
	  tagList(
		${0}
	  )
	}
	
	${1:name} <- function(input, output, session) {
	  
	}

#########################################
######Custom Snippets
#########################################

#dplyr
snippet my_dpipe
	gapminder %>%
		filter(year == 2007) %>%
		mutate(lifeExpMonths = 12 * lifeExp) %>%
		group_by(continent) %>%
		summarize(medianLE = median(lifeExpMonths), maxLE = max(lifeExpMonths)) %>%
		arrange(desc(continent))

snippet my_dtop5values
	${1:dataframe} %>% 
		filter(${2:filtervar} == "${3:searchterm}") %>%
		arrange(desc(${4:sortingvar})) %>% 
		select(${2:filtervar},${4:sortingvar})

snippet my_dtop5mostfrequent
	${1:dataframe} %>% 
		filter(is.na(${2:var}) == FALSE) %>% #slice_max does not work if column contains NAs
		group_by(${2:var}) %>%
		summarize(n=n()) %>% 
		slice_max(order_by = n, n=5) #Use slice_min for bottom 5

#ggplot
snippet my_gghisbasic
	ggplot(data = ${1:data}) +
	geom_histogram(mapping = aes(x = ${2:xvar})) +
	labs(x = "${3:xlabel}")

snippet my_gghisdens
	ggplot(${1:data},aes(x=${2:xvar})) +
	geom_histogram(aes(y=..density..), alpha=0.5) +
	geom_density( alpha = 0.2) +
	labs(x = "${3:xlabel}")
	
snippet my_ggboxplot
	ggplot(data = ${1:data}, aes(x="",y=${2:yvar})) + 
	geom_boxplot() + 
	geom_hline(yintercept = ${3:hline}) +
	stat_summary(fun = mean, geom = "point", shape = 20, size = 5, color = "red") +
	theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())
	
snippet my_ggqq
	ggplot(data = ${1:data}, aes(sample = ${2:var}, color = as.factor(${3:var}))) +
	stat_qq() +
	stat_qq_line()
	
snippet my_ggscatter
	ggplot(${1:data}, aes(x=${2:xvar}, y=${3:yvar}, color=${4:zvar}, size=${5:avar})) +
	geom_point() +
	scale_x_log10() + #Take the common log of the x or y axis
	facet_wrap(~${6:var}) + #Create different scatter plots by this variable
	geom_smooth(method=lm) #Add Regression line

snippet my_ggbar
	ggplot(${1:data}) + 
	geom_bar(aes(x=${2:xvar}, y=${3:yvar}), 
		position = "dodge", stat = "summary", fun = "mean")

#Model Selection
snippet my_modselectlinreg
#Create training model
	set.seed(19054)
	${1:dataframe} <- ${1:dataframe} %>% mutate(id = row_number()) #Only necessary if data does not have id-like column
	train = ${1:dataframe} %>% sample_frac(0.7) #70% of data points are set aside for training (can be changed)
	test <- anti_join(${1:dataframe}, train, by = 'id') #The remainder are assigned for testing
	train = train[,c(${2:colnumstovalidate})] #Select columns you want to analyze
	#train = train[,!(colnames(train) %in% "id")] #Remove id column if not used in modeling
	
	# Create full model and empty model
	full.model = lm(${3:yvar} ~ . , data = train)
	empty.model = lm(${3:yvar} ~ 1, data = train)
	
	# Information Criteria (InfCrit)
	#AIC: k = 2
	#BIC: k = log(nrow(train))
	#p-value: k = qchisq(alpha.f , 1, lower.tail = FALSE)
	
	#Create model
	#alpha.f=0.20 #Only needed for p-value calculation
	
	for.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "forward", k = ${5:InfCrit})
	back.model = step(full.model, scope = list(lower = empty.model, upper = full.model), direction = "backward", k = ${5:InfCrit})
	step.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "both", k = ${5:InfCrit})
	
snippet my_alpha
#Used for selecting alpha
	alpha = 1-pchisq(log(nrow(training)),1)
	
snippet my_createdf
#Create a blank dataframe and assign column names to the variables.
	${1:dataframe} = data.frame(matrix(ncol = ${2:ncols}, nrow = 0))
	colnames(${1:dataframe}) <- c('${3:var1}', '${4:var2}', '${5:var3}')

snippet my_eda
	#Includes a number of different scripts useful for Exploratory Data Analysis
	sapply(${1:dataframe}, function(x) sum(is.na(x))) #Number of NAs per variable

	#Specify a list of categorical variables to conver to a factor
	cols = c('${3:var1}', '${4:var2}', '${5:var3}')
	train[cols] = lapply(train[cols], factor)

	#Impute missing values with median (continuous variables) and mode (categorical variables)
	#Create indicator variables when imputation occurs for continuous variables
	library(imputeMissings)
	train = imputeMissings::impute(${1:dataframe}, object = NULL, method = "median/mode", flag = TRUE)
	
snippet my_normalityassumptions
	#install.packages('nortest')
	library(nortest)
	ad.test(${1:linregmodel}\$residuals)
	shapiro.test(${1:linregmodel}\$residuals)
	
	#Also can look at residuals graphically. Use the snippet my_ggqq
	
snippet my_writetocsv
	#Write data frame to .csv (stored in same location as code file)
	write.csv(${1:df}, file = '${2:filename}.csv', row.names = FALSE )
	
################################
#Flowsheet Resource Snippets
################################

snippet fdr_alpha
	#Determines alpha value based on sample size and BIC criterion
	alpha = 1-pchisq(log(nrow(${1:dataset})),1)
	
snippet fdr_linreg
	#Creates a linear regression model that is a function of all predictor variables
	lm.model = lm(${1:yvar} ~ . , data = ${2:dataset})

snippet fdr_globalF
	#Global F test: Determines if at least one variable is significant
	lm.model <- lm(${1:targetvar} ~ ${2:predvar1} + ${3:predvar2}, data = ${4:dataset})
	summary(lm.model)

snippet fdr_assumptions_linearity
	#Check to see if the below plot is random with an average value of zero
	library(ggplot2)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
		geom_point(color="blue")+
		labs(x="Predicted Values",y="Residuals")



snippet fdr_assumptions_constvar 
	#Check to see if the below plot has a constant spread over the range of predicted values
	library(ggplot2)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
		geom_point(color="blue") +
		labs(x="Predicted Values",y="Residuals")


snippet fdr_assumptions_normdist
	#Can use QQ-plot, Anderson-Darling test, or Shapiro-Wilk test
	library(ggplot2)
	library(nortest)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	
	ggplot(data = ${2:dataset}, aes(sample = lm.model\$residuals)) + #i. QQ-Plot
		stat_qq() +
		stat_qq_line()
		
	ad.test(lm.model\$residuals) #ii. Anderson-Darling test. Gives more weight to tails
	shapiro.test(lm.model\$residuals) #iii. Shapiro-Wilk test. Better for smaller data sets


snippet fdr_assumptions_independence
	#Can use plot to visually see time-dependence or use Durbin-Watson test
	library(ggplot2)
	library(lmtest)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})

	ggplot(train,aes(x=${3:timevariable},y=lm.model\$residuals)) +  #i. Visual Plot
		geom_point(color="blue") +
		labs( x="Time",y="Residuals")

	lm.model=lm(${1:targetvar}~${3:timevariable},data=${2:dataset})
	dwtest(lm.model,alternative="greater") #ii. Durbin-Watson test


snippet fdr_assumptions_multicol
	#Check for correlation among predictor variables or calculate the variance inflation factor (VIF)
	library(car)
	
	#First filter dataframe to only contain numeric variables
	${1:dataset}_num = ${1:dataset}[,c(5,6,82)]
	cor(${1:dataset}_num) #i. Correlation among predictor variables
	
	lm.model=lm(${2:targetvar}~.,data=${1:dataset})
	vif(lm.model) #ii. Variance inflation factor (VIF)


snippet fdr_outliers
	#Tests for finding outliers
	library(ggplot2)
	library(pdp)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	n.index = seq(1, nrow(${2:dataset}))
	
	#i. Internal Studentized Residuals
	a = ggplot(lm.model,aes(x=n.index,y=rstandard(lm.model))) + 
		geom_point(color="orange") + 
		geom_line(y=-3) + 
		geom_line(y=3) + 
		labs(title = "Internal Studentized Residuals",x="Observation",y="Residuals")
	
	#ii. External Studentized Residuals
	b = ggplot(lm.model,aes(x=n.index,y=rstudent(lm.model)))+ 
		geom_point(color="orange") + 
		geom_line(y=-3) + 
		geom_line(y=3) + 
		labs(title = "External Studentized Residuals",x="Observation",y="Residuals")
	
	grid.arrange(a,b)

	
snippet fdr_influential
	#Tests for finding influential observations
	library(ggplot2)
	library(pdp)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	n.index = seq(1, nrow(${2:dataset}))
	p = length(lm.model\$coefficients)
	
	#i. Cook's D
	D.cut= 4/(nrow(${2:dataset})-p-1)
	
	c = ggplot(lm.model,aes(x=n.index,y=cooks.distance(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=D.cut) +
				labs(title = "Cook's D",x = "Observation",y ="Cook's Distance")

	#ii. DFFITS
	df.cut=2*(sqrt((p)/nrow(${2:dataset})))
	
	d = ggplot(lm.model,aes(x=n.index,y=dffits(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=df.cut) +
				geom_line(y=-df.cut) +
				labs(title = "DFFITS",x="Observation",y="DFFITS")

	#Observations that are outside of dffits
	${2:dataset}[dffits(lm.model) > df.cut | dffits(lm.model) < -df.cut,]
	
	#iii. DFBETAS (copy and paste for each variable)
	db.cut=2/sqrt(nrow(${2:dataset}))

	e = ggplot(lm.model,aes(x=n.index,y=dfbetas(lm.model)[,'${3:variablename}'])) + 
				geom_point(color="orange") + 
				geom_line(y=db.cut) + 
				geom_line(y=-db.cut) + 
				labs(title = "DFBETA for ${3:variablename}",x="Observation",y="DFBETAS")
	
	#iv. Hhat
	hat.cut=2*(p+1)/nrow(${2:dataset})
	
	f = ggplot(lm.model,aes(x=n.index,y=hatvalues(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=hat.cut) +
				labs(title = "Hat values",x="Observation",y="Hat Values")

	#Print all plots in a grid. Add additional variables for DFBETAS if desired
	grid.arrange(c,d,e,f)

snippet fdr_varselect_linreg
	#Create training model
	library(dplyr)
	library(glmnet)
	set.seed(19054)
	
	${1:dataframe} <- ${1:dataframe} %>% mutate(id = row_number()) #Only necessary if data does not have id-like column
	train = ${1:dataframe} %>% sample_frac(0.7) #70% of data points are set aside for training (can be changed)
	test <- anti_join(${1:dataframe}, train, by = 'id') #The remainder are assigned for testing
	train = train[,c(${2:colnumstovalidate})] #Select columns you want to analyze
	#train = train[,!(colnames(train) %in% "id")] #Remove id column if not used in modeling
	
	# Create full model and empty model
	full.model = lm(${3:yvar} ~ . , data = train)
	empty.model = lm(${3:yvar} ~ 1, data = train)
	
	# Information Criteria (InfCrit)
	#AIC: k = 2
	#BIC: k = log(nrow(train))
	#p-value: k = qchisq(alpha.f , 1, lower.tail = FALSE)
	
	#Create model
	#alpha.f=0.20 #Only needed for p-value calculation
	
	for.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "forward", k = ${5:InfCrit})
	back.model = step(full.model, scope = list(lower = empty.model, upper = full.model), direction = "backward", k = ${5:InfCrit})
	step.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "both", k = ${5:InfCrit})
	
	#Regularized regression for variable selection (LASSO example)
	train_x <- model.matrix(${3:yvar} ~ ., data = train)[, -1]
	train_y <- train\$'${3:yvar}'
	
	train_lasso = glmnet(x=train_x, y = ${3:yvar}, alpha = 1)
	plot(train_lasso, xvar = "lambda")

snippet fdr_modeval
	#Evaluate various metrics for model accuracy
	library(dplyr)
	
	${1:testdataset}\$pred_lm <- predict(${2:finalmodel}, newdata = ${1:testdataset})
	
	${1:testdataset} %>% 
	  mutate(lm_APE = 100*abs((${3:yvar} - pred_lm)/${3:yvar})) %>% 
	  mutate(lm_AE = abs(${3:yvar} - pred_lm)) %>% 
	  mutate(lm_SE = (${3:yvar} - pred_lm)^2) %>% 
	  dplyr::summarise(MAPE_lm = mean(lm_APE)
															, MAE_lm = mean(lm_AE)
															, MSE_lm = mean(lm_SE)
															, RMSE_lm = sqrt(mean(lm_SE))
															, AIC_lm = AIC(${2:finalmodel})
															, BIC_lm = BIC(${2:finalmodel})
															, Ra2_lm = summary(${2:finalmodel})\$adj.r.squared)